{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical Session - Transformers\n",
    "---\n",
    "\n",
    "### Goal  \n",
    "The goal of this session is to implement a standard Transformer in PyTorch from scratch.\n",
    "\n",
    "### Task  \n",
    "As a sample problem, we will focus on sorting a list of digits from 1 to 20.\n",
    "\n",
    "Example:\n",
    "Source Sequence: `[19, 7, 2, 9, 18]`\n",
    "Target Output:   `[2, 7, 9, 18, 19]`\n",
    "\n",
    "### Outline\n",
    "1. Embed the tokens/numbers into vectors\n",
    "2. The `Transformer` layer\n",
    "- 2.1. Implement the self-attention layer, as seen in class\n",
    "- 2.2 Integrate the self-attention layer in a transformer layer\n",
    "3. Implement a `Transformer` network\n",
    "4. Train the network!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 0. Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example source sequence and target:\n",
      "--------------------------------------------------\n",
      "Input Sequence:         [19, 7, 2, 9, 18]\n",
      "Expected Sorted Output: [2, 7, 9, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "VOCAB_SIZE = 20  # Numbers from 1 to vocab_size\n",
    "SEQ_LENGTH = 5   # Sequence length\n",
    "\n",
    "def generate_data(batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Generates random sequences of integers and their sorted counterparts.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of sequences to generate.\n",
    "        seq_length (int): Length of each sequence.\n",
    "        vocab_size (int): Maximum integer value (exclusive) for the sequence elements.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: \n",
    "            - src (torch.Tensor): A tensor of shape (batch_size, seq_length) containing \n",
    "              random integers in the range [1, vocab_size).\n",
    "            - tgt (torch.Tensor): A tensor of shape (batch_size, seq_length) containing \n",
    "              the sorted version of each sequence in `src`.\n",
    "    \"\"\"\n",
    "    src = torch.randint(1, vocab_size, (batch_size, seq_length))\n",
    "    tgt = torch.sort(src, dim=1)[0] \n",
    "    return src, tgt\n",
    "\n",
    "source, target = generate_data(1, SEQ_LENGTH, VOCAB_SIZE)\n",
    "print(\"Example source sequence and target:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Input Sequence:         {source.tolist()[0]}\")\n",
    "print(f\"Expected Sorted Output: {target.tolist()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Embed the tokens into vectors\n",
    "\n",
    "First step is to transform the input integers into vectors of a fixed dimension `d`\n",
    "##### How to do this:  \n",
    "1. **Token Embeddings**: Each input token (integer index) is mapped to a high-dimensional vector using [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).  \n",
    "2. **Positional Encoding**: Instead of the classical sine-cosine positional encodings, we simply use a learnable vector for each position in the sequence, again using `torch.nn.Embedding`.\n",
    "3. **Summation**: The final embedding is the sum of token embeddings and positional encodings.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded batch of sequences [[2, 16, 17, 5, 10, 8, 6, 10, 19, 12], [12, 14, 17, 13, 8, 1, 17, 10, 5, 5], [16, 6, 3, 8, 19, 1, 17, 15, 8, 15]] into:\n",
      "Tensor of shape  torch.Size([3, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class IntegerSequenceEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding module that combines token embeddings with positional encodings.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary (number of unique tokens).\n",
    "        embed_dim (int): Dimension of the embeddings.\n",
    "        seq_length (int): Sequence length.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=20, embed_dim=16, seq_length=5):\n",
    "        super().__init__()\n",
    "        # embedding layer for the tokens (numbers):\n",
    "        self.token_embedding = nn.Embedding(...)\n",
    "        # embedding layer for the positions:\n",
    "        self.positional_embedding = nn.Embedding(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the embedding module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length), \n",
    "                              containing integer token indices.\n",
    "        Returns:\n",
    "            torch.Tensor: Embedded tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        \"\"\"\n",
    "        # Token embedding\n",
    "        x = ...  # Shape: (batch_size, seq_length, embed_dim)\n",
    "        # Positional encoding\n",
    "        positions = ...              # Shape: (1, seq_length)\n",
    "        x = x + self.positional_embedding(positions)\n",
    "\n",
    "        return x\n",
    "    \n",
    "embedding_layer = IntegerSequenceEmbedding(vocab_size=21, embed_dim=32, seq_length=20)\n",
    "src, _ = generate_data(batch_size=3, seq_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "embedded_src = embedding_layer(src)\n",
    "print(f\"Embedded batch of sequences {src.tolist()} into:\")\n",
    "print(f\"Tensor of shape  {embedded_src.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: The `Transformer` layer\n",
    "\n",
    "#### 2.1. Implement the self-attention layer, as seen in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(...)  # Query projection\n",
    "        self.W_k = nn.Linear(...)  # Key projection\n",
    "        self.W_v = nn.Linear(...)  # Value projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: (batch_size, seq_length, embed_dim)\n",
    "        # TODO: Complete this function\n",
    "        Q = ...  # (batch_size, seq_length, embed_dim)\n",
    "        K = ...  # (batch_size, seq_length, embed_dim)\n",
    "        V = ...  # (batch_size, seq_length, embed_dim)\n",
    "        d_k = K.shape[-1] # Key dimension\n",
    "\n",
    "\n",
    "        # Dot-product similarities\n",
    "        scores = ...\n",
    "        # Scale by dimension\n",
    "        scores = ...            \n",
    "        # Transform the scores into probabilities with the softmax function\n",
    "        scores = ...\n",
    "    \n",
    "        # Optional: store the attention weights for visualization\n",
    "        self.attention_weights = scores\n",
    "\n",
    "        # Update the vectors x\n",
    "        x = ...\n",
    "\n",
    "        return x\n",
    "\n",
    "# Testing\n",
    "attn = SingleHeadAttention(embed_dim=128)  \n",
    "x = torch.randn(32, 10, 128)  # Batch of 32 sequences, each of length 10 with 128-d embeddings  \n",
    "output = attn(x)  \n",
    "print(output.shape)  # Should be (32, 10, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Integrate the self-attention layer in a transformer layer\n",
    "\n",
    "A **Transformer Encoder Layer** consists of:  \n",
    "- A *self-attention mechanism* to capture long-range dependencies.  \n",
    "- *Fully connected (feedforward) layers* to transform representations.  \n",
    "- *Layer normalization* to stabilize training.  \n",
    "- *Residual connections* to improve gradient flow and prevent vanishing gradients.\n",
    "<div style=\"max-width:400px\">\n",
    "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5024bcc5-33c9-4d53-9bd7-56cbcf9c4627_874x1108.png\" alt=\"Transformer Layer\" />\n",
    "<div/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.self_attn = SingleHeadAttention(embed_dim)\n",
    "        # Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 2, embed_dim)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: (batch_size, seq_length, embed_dim)\n",
    "        # TODO: Implement encoder block, with residual connections!\n",
    "        ...\n",
    "        return x\n",
    "    \n",
    "# Testing\n",
    "attn = TransformerEncoderLayer(embed_dim=128)\n",
    "x = torch.randn(32, 10, 128)  # Batch of 32 sequences, each of length 10 with 128-d embeddings  \n",
    "output = attn(x)  \n",
    "print(output.shape)  # Should be (32, 10, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Implement a `Transformer` network\n",
    "\n",
    "#### 3.1. General architecture:\n",
    "The full Transformer network consists of:  \n",
    "1. **Embedding Module**: Converts input tokens into dense vectors and adds positional encodings.  \n",
    "2. **Transformer Layers**: A stack of self-attention layers with feedforward networks and normalization.  \n",
    "\n",
    "<div style=\"max-width:600px\">\n",
    "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6133c18-bfaf-4578-8c5a-e5ac7809f65b_1632x784.png\" alt=\"Transformer Architecture, with zoom on transformer layer\", \"width=\"50px\"\\>\n",
    "</div>\n",
    "\n",
    "3. **Classification Head**: Processes the output of the Transformer layers to produce predictions.\n",
    "\n",
    "#### 3.2. Predictions for our task\n",
    "\n",
    "The task is to **sort a list of integers**. What should be the output of the model? Of what dimension is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence:         [10, 10, 14, 1, 8, 10, 6, 7, 12, 7]\n",
      "Expected Sorted Output: [1, 6, 7, 7, 8, 10, 10, 10, 12, 14]\n",
      "Model Prediction:       [2, 15, 0, 8, 10, 15, 3, 14, 11, 14]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, seq_length=5, num_layers=2):\n",
    "        \"\"\"\n",
    "        Transformer Encoder for sequence processing.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique tokens in the input vocabulary.\n",
    "            embed_dim (int): Dimension of the token embeddings.\n",
    "            num_layers (int): Number of Transformer encoder layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Token embedding layer\n",
    "        self.embedding = ...\n",
    "\n",
    "        # Stack of Transformer Encoder Layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            ...\n",
    "        ])\n",
    "\n",
    "        # Final classification head: a simple linear layer\n",
    "        self.fc_out = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer Encoder.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_length).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, seq_length, vocab_size) containing probabilities for each token.\n",
    "        \"\"\"\n",
    "        # Convert input sequence to embeddings\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Pass through Transformer Encoder Layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Apply final linear layer to get logits\n",
    "        logits = ...\n",
    "\n",
    "        return logits\n",
    "\n",
    "embed_dim = 32\n",
    "batch_size = 16\n",
    "transformer = Transformer(VOCAB_SIZE, embed_dim=embed_dim, seq_length=SEQ_LENGTH)\n",
    "# Generate source and target data\n",
    "source, target = generate_data(batch_size, SEQ_LENGTH, VOCAB_SIZE)\n",
    "\n",
    "# Pass the source data through the transformer and check the output shape\n",
    "logits = transformer(source)\n",
    "predictions = ... # predictions should be a list of integers, the same length as source.\n",
    "print(f\"Input Sequence:         {source.tolist()[0]}\")\n",
    "print(f\"Expected Sorted Output: {target.tolist()[0]}\")\n",
    "print(f\"Model Prediction:       {predictions.tolist()[0]}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Train the network!\n",
    "\n",
    "As for other neural networks, the Transformer parameters are learned by stochastic gradient descent on a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Input Sequence:         [3, 8, 5, 19, 16, 14, 10, 6, 6, 13]\n",
      "Expected Sorted Output: [3, 5, 6, 6, 8, 10, 13, 14, 16, 19]\n",
      "Model Prediction:       [2, 5, 5, 8, 5, 5, 8, 5, 5, 5]\n",
      "Loss: 3.2324\n",
      "--------------------------------------------------\n",
      "Epoch 10\n",
      "Input Sequence:         [7, 13, 12, 9, 6, 3, 7, 9, 5, 11]\n",
      "Expected Sorted Output: [3, 5, 6, 7, 7, 9, 9, 11, 12, 13]\n",
      "Model Prediction:       [2, 3, 7, 7, 10, 7, 10, 19, 17, 19]\n",
      "Loss: 2.2913\n",
      "--------------------------------------------------\n",
      "Epoch 50\n",
      "Input Sequence:         [13, 14, 3, 9, 3, 14, 15, 5, 6, 6]\n",
      "Expected Sorted Output: [3, 3, 5, 6, 6, 9, 13, 14, 14, 15]\n",
      "Model Prediction:       [3, 3, 5, 6, 8, 8, 13, 14, 14, 15]\n",
      "Loss: 1.4215\n",
      "--------------------------------------------------\n",
      "Epoch 100\n",
      "Input Sequence:         [12, 3, 3, 3, 11, 14, 7, 3, 16, 16]\n",
      "Expected Sorted Output: [3, 3, 3, 3, 7, 11, 12, 14, 16, 16]\n",
      "Model Prediction:       [3, 3, 3, 3, 3, 11, 11, 14, 16, 16]\n",
      "Loss: 0.7935\n",
      "--------------------------------------------------\n",
      "Epoch 150\n",
      "Input Sequence:         [14, 13, 18, 10, 13, 5, 16, 6, 10, 13]\n",
      "Expected Sorted Output: [5, 6, 10, 10, 13, 13, 13, 14, 16, 18]\n",
      "Model Prediction:       [5, 6, 10, 10, 13, 13, 13, 14, 16, 18]\n",
      "Loss: 0.4332\n",
      "--------------------------------------------------\n",
      "Epoch 200\n",
      "Input Sequence:         [16, 5, 5, 3, 16, 4, 19, 18, 7, 3]\n",
      "Expected Sorted Output: [3, 3, 4, 5, 5, 7, 16, 16, 18, 19]\n",
      "Model Prediction:       [3, 3, 4, 5, 5, 7, 16, 16, 18, 19]\n",
      "Loss: 0.3544\n",
      "--------------------------------------------------\n",
      "Epoch 250\n",
      "Input Sequence:         [11, 9, 6, 2, 19, 18, 16, 17, 17, 15]\n",
      "Expected Sorted Output: [2, 6, 9, 11, 15, 16, 17, 17, 18, 19]\n",
      "Model Prediction:       [2, 6, 9, 11, 15, 15, 17, 17, 18, 19]\n",
      "Loss: 0.2420\n",
      "--------------------------------------------------\n",
      "Epoch 300\n",
      "Input Sequence:         [1, 14, 4, 12, 6, 2, 12, 11, 6, 12]\n",
      "Expected Sorted Output: [1, 2, 4, 6, 6, 11, 12, 12, 12, 14]\n",
      "Model Prediction:       [1, 2, 4, 6, 6, 12, 12, 12, 12, 14]\n",
      "Loss: 0.1477\n",
      "--------------------------------------------------\n",
      "Epoch 350\n",
      "Input Sequence:         [3, 4, 6, 1, 1, 1, 2, 15, 19, 9]\n",
      "Expected Sorted Output: [1, 1, 1, 2, 3, 4, 6, 9, 15, 19]\n",
      "Model Prediction:       [1, 1, 1, 1, 3, 4, 6, 6, 15, 19]\n",
      "Loss: 0.1235\n",
      "--------------------------------------------------\n",
      "Epoch 400\n",
      "Input Sequence:         [12, 15, 10, 11, 14, 4, 6, 9, 19, 7]\n",
      "Expected Sorted Output: [4, 6, 7, 9, 10, 11, 12, 14, 15, 19]\n",
      "Model Prediction:       [4, 7, 7, 9, 10, 11, 12, 14, 15, 19]\n",
      "Loss: 0.1262\n",
      "--------------------------------------------------\n",
      "Epoch 450\n",
      "Input Sequence:         [16, 11, 1, 13, 10, 15, 15, 13, 16, 8]\n",
      "Expected Sorted Output: [1, 8, 10, 11, 13, 13, 15, 15, 16, 16]\n",
      "Model Prediction:       [1, 8, 10, 11, 13, 13, 15, 15, 16, 16]\n",
      "Loss: 0.1065\n",
      "--------------------------------------------------\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "vocab_size = 20 \n",
    "seq_length = 10\n",
    "# Network hyperparameters\n",
    "embed_dim = 32\n",
    "num_layers = 2\n",
    "# Training hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = Transformer(vocab_size, embed_dim=embed_dim, seq_length=seq_length)\n",
    "criterion = ...\n",
    "optimizer = ...\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Generate a mini-batch for training\n",
    "    src, tgt = generate_data(batch_size, seq_length, vocab_size)\n",
    "    # Forward pass\n",
    "    output = ...\n",
    "    loss = ...\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Parameter updates\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print model output at each epoch\n",
    "    if epoch % 50 == 0 or epoch==10:\n",
    "        test_src, test_tgt = generate_data(batch_size, seq_length, vocab_size)\n",
    "        # test_pred should be a list of integers, the same length as test_src.\n",
    "        test_pred = ...\n",
    "\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\"Input Sequence:         {test_src.tolist()[0]}\")\n",
    "        print(f\"Expected Sorted Output: {test_tgt.tolist()[0]}\")\n",
    "        print(f\"Model Prediction:       {test_pred.tolist()[0]}\")\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence 1:         [6, 13, 3, 14, 7, 3, 4, 5, 18, 2]\n",
      "Expected Sorted Output 1: [2, 3, 3, 4, 5, 6, 7, 13, 14, 18]\n",
      "Model Prediction 1:       [2, 3, 3, 4, 5, 6, 7, 13, 14, 18]\n",
      "--------------------------------------------------\n",
      "Input Sequence 2:         [3, 4, 13, 12, 6, 11, 7, 4, 3, 16]\n",
      "Expected Sorted Output 2: [3, 3, 4, 4, 6, 7, 11, 12, 13, 16]\n",
      "Model Prediction 2:       [3, 3, 4, 4, 6, 7, 11, 12, 13, 16]\n",
      "--------------------------------------------------\n",
      "Input Sequence 3:         [13, 1, 9, 3, 8, 3, 2, 15, 17, 15]\n",
      "Expected Sorted Output 3: [1, 2, 3, 3, 8, 9, 13, 15, 15, 17]\n",
      "Model Prediction 3:       [1, 2, 3, 3, 9, 9, 13, 15, 15, 17]\n",
      "--------------------------------------------------\n",
      "Input Sequence 4:         [18, 13, 17, 11, 14, 7, 4, 18, 17, 18]\n",
      "Expected Sorted Output 4: [4, 7, 11, 13, 14, 17, 17, 18, 18, 18]\n",
      "Model Prediction 4:       [4, 7, 11, 13, 14, 17, 17, 18, 18, 18]\n",
      "--------------------------------------------------\n",
      "Input Sequence 5:         [16, 13, 16, 1, 8, 17, 18, 11, 5, 2]\n",
      "Expected Sorted Output 5: [1, 2, 5, 8, 11, 13, 16, 16, 17, 18]\n",
      "Model Prediction 5:       [1, 2, 5, 8, 11, 13, 16, 16, 17, 18]\n",
      "--------------------------------------------------\n",
      "Loss: 0.0970\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the model on 5 example sequences\n",
    "for i in range(5):\n",
    "    print(f\"Input Sequence {i + 1}:         {test_src.tolist()[i]}\")\n",
    "    print(f\"Expected Sorted Output {i + 1}: {test_tgt.tolist()[i]}\")\n",
    "    print(f\"Model Prediction {i + 1}:       {test_pred.tolist()[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Implement multi-headed attention\n",
    "\n",
    "<div style=\"max-width:400px\">\n",
    "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65c156ae-5cc5-4f7f-8652-dd5311b19beb_544x724.png\" alt=\"Transformer Architecture, with zoom on transformer layer\", \"width=\"50px\"\\>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__inita__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"Embedding size must be divisible by num_heads\"\n",
    "\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)  # Query projection\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)  # Key projection\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)  # Value projection\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "\n",
    "        # TODO: Compute Queries, Keys, Values\n",
    "        ...\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
