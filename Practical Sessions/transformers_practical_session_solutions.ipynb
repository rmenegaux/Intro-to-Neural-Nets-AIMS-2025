{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical Session - Transformers\n",
    "---\n",
    "\n",
    "### Goal  \n",
    "The goal of this session is to implement a standard Transformer in PyTorch from scratch.\n",
    "\n",
    "### Task  \n",
    "As a sample problem, we will focus on sorting a list of digits from 1 to 20.\n",
    "\n",
    "Example:\n",
    "Source Sequence: `[19, 7, 2, 9, 18]`\n",
    "Target Output:   `[2, 7, 9, 18, 19]`\n",
    "\n",
    "### Outline\n",
    "1. Embed the tokens/numbers into vectors\n",
    "2. The `Transformer` layer\n",
    "- 2.1. Implement the self-attention layer, as seen in class\n",
    "- 2.2 Integrate the self-attention layer in a transformer layer\n",
    "3. Implement a `Transformer` network\n",
    "4. Train the network!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 0. Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example source sequence and target:\n",
      "--------------------------------------------------\n",
      "Input Sequence:         [2, 1, 19, 17, 16]\n",
      "Expected Sorted Output: [1, 2, 16, 17, 19]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "VOCAB_SIZE = 20  # Numbers from 1 to vocab_size\n",
    "SEQ_LENGTH = 5   # Sequence length\n",
    "\n",
    "def generate_data(batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Generates random sequences of integers and their sorted counterparts.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of sequences to generate.\n",
    "        seq_length (int): Length of each sequence.\n",
    "        vocab_size (int): Maximum integer value (exclusive) for the sequence elements.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: \n",
    "            - src (torch.Tensor): A tensor of shape (batch_size, seq_length) containing \n",
    "              random integers in the range [1, vocab_size).\n",
    "            - tgt (torch.Tensor): A tensor of shape (batch_size, seq_length) containing \n",
    "              the sorted version of each sequence in `src`.\n",
    "    \"\"\"\n",
    "    src = torch.randint(1, vocab_size, (batch_size, seq_length))\n",
    "    tgt = torch.sort(src, dim=1)[0] \n",
    "    return src, tgt\n",
    "\n",
    "source, target = generate_data(1, SEQ_LENGTH, VOCAB_SIZE)\n",
    "print(\"Example source sequence and target:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Input Sequence:         {source.tolist()[0]}\")\n",
    "print(f\"Expected Sorted Output: {target.tolist()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Embed the tokens into vectors\n",
    "\n",
    "First step is to transform the input integers into vectors of a fixed dimension `d`\n",
    "##### How to do this:  \n",
    "1. **Token Embeddings**: Each input token (integer index) is mapped to a high-dimensional vector using [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).  \n",
    "2. **Positional Encoding**: Instead of the classical sine-cosine positional encodings, we simply use a learnable vector for each position in the sequence, again using `torch.nn.Embedding`.\n",
    "3. **Summation**: The final embedding is the sum of token embeddings and positional encodings.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(SEQ_LENGTH).view(1, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded batch of sequences [[7, 2, 13, 4, 18]] into:\n",
      "Tensor of shape  torch.Size([1, 5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3827,  0.0612,  0.3681],\n",
       "         [-0.7669,  1.0268,  0.7466],\n",
       "         [-1.2059,  0.1586, -1.3335],\n",
       "         [ 1.3124, -3.4395,  0.5323],\n",
       "         [ 0.5295,  0.5688, -1.6422]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class IntegerSequenceEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding module that combines token embeddings with positional encodings.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary (number of unique tokens).\n",
    "        embed_dim (int): Dimension of the embeddings.\n",
    "        seq_length (int): Sequence length.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=20, embed_dim=16, seq_length=5):\n",
    "        super().__init__()\n",
    "        # embedding layer for the tokens (numbers):\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # embedding layer for the positions:\n",
    "        self.positional_embedding = nn.Embedding(seq_length, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the embedding module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length), \n",
    "                              containing integer token indices.\n",
    "        Returns:\n",
    "            torch.Tensor: Embedded tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        \"\"\"\n",
    "        # Token embedding\n",
    "        x = self.token_embedding(x)  # Shape: (batch_size, seq_length, embed_dim)\n",
    "        # Positional encoding\n",
    "        positions = torch.arange(x.shape[1]).unsqueeze(0)              # Shape: (1, seq_length)\n",
    "        x = x + self.positional_embedding(positions)\n",
    "\n",
    "        return x\n",
    "    \n",
    "embedding_layer = IntegerSequenceEmbedding(vocab_size=21, embed_dim=3, seq_length=SEQ_LENGTH)\n",
    "src, _ = generate_data(batch_size=1, seq_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
    "embedded_src = embedding_layer(src)\n",
    "print(f\"Embedded batch of sequences {src.tolist()} into:\")\n",
    "print(f\"Tensor of shape  {embedded_src.shape}\")\n",
    "embedded_src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: The `Transformer` layer\n",
    "\n",
    "#### 2.1. Implement the self-attention layer, as seen in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)  # Query projection\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)  # Key projection\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)  # Value projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: (batch_size, seq_length, embed_dim)\n",
    "        # TODO: Complete this function\n",
    "        Q = self.W_q(x)  # (batch_size, seq_length, embed_dim)\n",
    "        K = self.W_k(x)  # (batch_size, seq_length, embed_dim)\n",
    "        V = self.W_v(x)  # (batch_size, seq_length, embed_dim)\n",
    "        d_k = K.shape[-1] # Key dimension\n",
    "\n",
    "\n",
    "        # Dot-product similarities\n",
    "        scores = Q @ K.transpose(1, 2)\n",
    "        # Scale by dimension\n",
    "        scores /= d_k ** 0.5            \n",
    "        # Transform the scores into probabilities with the softmax function\n",
    "        scores = torch.softmax(scores, dim=-1)\n",
    "    \n",
    "        # Optional: store the attention weights for visualization\n",
    "        self.attention_weights = scores\n",
    "\n",
    "        # Update the vectors x\n",
    "        x = scores @ V\n",
    "\n",
    "        return x\n",
    "\n",
    "# Testing\n",
    "attn = SingleHeadAttention(embed_dim=128)  \n",
    "x = torch.randn(32, 10, 128)  # Batch of 32 sequences, each of length 10 with 128-d embeddings  \n",
    "output = attn(x)  \n",
    "print(output.shape)  # Should be (32, 10, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]])\n",
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n"
     ]
    }
   ],
   "source": [
    "# Checking the softmax dimension\n",
    "A = torch.arange(9).view(1,3,3).float()\n",
    "print(A[0])\n",
    "print(torch.softmax(A, dim=2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Integrate the self-attention layer in a transformer layer\n",
    "\n",
    "A **Transformer Encoder Layer** consists of:  \n",
    "- A *self-attention mechanism* to capture long-range dependencies.  \n",
    "- *Fully connected (feedforward) layers* to transform representations.  \n",
    "- *Layer normalization* to stabilize training.  \n",
    "- *Residual connections* to improve gradient flow and prevent vanishing gradients.\n",
    "<div style=\"max-width:400px\">\n",
    "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5024bcc5-33c9-4d53-9bd7-56cbcf9c4627_874x1108.png\" alt=\"Transformer Layer\" />\n",
    "<div/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.self_attn = SingleHeadAttention(embed_dim)\n",
    "        # Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 2, embed_dim)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: (batch_size, seq_length, embed_dim)\n",
    "        # TODO: Implement encoder block, with residual connections!\n",
    "        \n",
    "        x = x + self.self_attn(self.norm1(x))\n",
    "        x = x + self.fc_layers(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Testing\n",
    "attn = TransformerEncoderLayer(embed_dim=128)\n",
    "x = torch.randn(32, 10, 128)  # Batch of 32 sequences, each of length 10 with 128-d embeddings  \n",
    "output = attn(x)  \n",
    "print(output.shape)  # Should be (32, 10, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Implement a `Transformer` network\n",
    "\n",
    "#### 3.1. General architecture:\n",
    "The full Transformer network consists of:  \n",
    "1. **Embedding Module**: Converts input tokens into dense vectors and adds positional encodings.  \n",
    "2. **Transformer Layers**: A stack of self-attention layers with feedforward networks and normalization.  \n",
    "\n",
    "<div style=\"max-width:600px\">\n",
    "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6133c18-bfaf-4578-8c5a-e5ac7809f65b_1632x784.png\" alt=\"Transformer Architecture, with zoom on transformer layer\", \"width=\"50px\"\\>\n",
    "</div>\n",
    "\n",
    "3. **Classification Head**: Processes the output of the Transformer layers to produce predictions.\n",
    "\n",
    "#### 3.2. Predictions for our task\n",
    "\n",
    "The task is to **sort a list of integers**. What should be the output of the model? Of what dimension is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence:         [18, 13, 15, 10, 16]\n",
      "Expected Sorted Output: [10, 13, 15, 16, 18]\n",
      "Model Prediction:       [10, 7, 2, 9, 13]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, seq_length=5, num_layers=2):\n",
    "        \"\"\"\n",
    "        Transformer Encoder for sequence processing.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique tokens in the input vocabulary.\n",
    "            embed_dim (int): Dimension of the token embeddings.\n",
    "            num_layers (int): Number of Transformer encoder layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Token embedding layer\n",
    "        self.embedding = IntegerSequenceEmbedding(vocab_size, embed_dim, seq_length)\n",
    "\n",
    "        # Stack of Transformer Encoder Layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final classification head: a simple linear layer\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer Encoder.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_length).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, seq_length, vocab_size) containing probabilities for each token.\n",
    "        \"\"\"\n",
    "        # Convert input sequence to embeddings\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Pass through Transformer Encoder Layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Apply final linear layer to get logits\n",
    "        outputs = self.fc_out(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "embed_dim = 32\n",
    "batch_size = 16\n",
    "transformer = Transformer(VOCAB_SIZE, embed_dim=embed_dim, seq_length=SEQ_LENGTH)\n",
    "# Generate source and target data\n",
    "source, target = generate_data(batch_size, SEQ_LENGTH, VOCAB_SIZE)\n",
    "\n",
    "# Pass the source data through the transformer and check the output shape\n",
    "outputs = transformer(source)\n",
    "predictions = outputs.argmax(dim=-1) # predictions should be a list of integers, the same length as source.\n",
    "print(f\"Input Sequence:         {source.tolist()[0]}\")\n",
    "print(f\"Expected Sorted Output: {target.tolist()[0]}\")\n",
    "print(f\"Model Prediction:       {predictions.tolist()[0]}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Train the network!\n",
    "\n",
    "As for other neural networks, the Transformer parameters are learned by stochastic gradient descent on a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Input Sequence:         [1, 11, 16, 10, 2, 5, 9, 3, 1, 7]\n",
      "Expected Sorted Output: [1, 1, 2, 3, 5, 7, 9, 10, 11, 16]\n",
      "Model Prediction:       [6, 1, 6, 15, 17, 15, 13, 16, 17, 8]\n",
      "Loss: 3.2640\n",
      "--------------------------------------------------\n",
      "Epoch 10\n",
      "Input Sequence:         [14, 7, 6, 10, 5, 8, 14, 12, 9, 17]\n",
      "Expected Sorted Output: [5, 6, 7, 8, 9, 10, 12, 14, 14, 17]\n",
      "Model Prediction:       [1, 3, 5, 6, 7, 8, 13, 13, 17, 19]\n",
      "Loss: 2.0396\n",
      "--------------------------------------------------\n",
      "Epoch 50\n",
      "Input Sequence:         [16, 17, 2, 7, 12, 18, 6, 3, 15, 19]\n",
      "Expected Sorted Output: [2, 3, 6, 7, 12, 15, 16, 17, 18, 19]\n",
      "Model Prediction:       [2, 3, 7, 8, 12, 16, 17, 17, 18, 18]\n",
      "Loss: 1.1139\n",
      "--------------------------------------------------\n",
      "Epoch 100\n",
      "Input Sequence:         [10, 12, 12, 19, 4, 9, 12, 11, 17, 2]\n",
      "Expected Sorted Output: [2, 4, 9, 10, 11, 12, 12, 12, 17, 19]\n",
      "Model Prediction:       [2, 4, 7, 9, 10, 12, 12, 12, 17, 19]\n",
      "Loss: 0.6212\n",
      "--------------------------------------------------\n",
      "Epoch 150\n",
      "Input Sequence:         [9, 11, 5, 5, 11, 18, 11, 8, 16, 19]\n",
      "Expected Sorted Output: [5, 5, 8, 9, 11, 11, 11, 16, 18, 19]\n",
      "Model Prediction:       [5, 5, 8, 9, 9, 11, 11, 16, 16, 19]\n",
      "Loss: 0.4009\n",
      "--------------------------------------------------\n",
      "Epoch 200\n",
      "Input Sequence:         [13, 6, 17, 14, 18, 13, 15, 1, 12, 14]\n",
      "Expected Sorted Output: [1, 6, 12, 13, 13, 14, 14, 15, 17, 18]\n",
      "Model Prediction:       [1, 6, 12, 13, 13, 14, 14, 16, 17, 18]\n",
      "Loss: 0.2812\n",
      "--------------------------------------------------\n",
      "Epoch 250\n",
      "Input Sequence:         [9, 1, 18, 10, 16, 3, 15, 1, 11, 15]\n",
      "Expected Sorted Output: [1, 1, 3, 9, 10, 11, 15, 15, 16, 18]\n",
      "Model Prediction:       [1, 1, 3, 9, 10, 11, 15, 15, 16, 18]\n",
      "Loss: 0.2077\n",
      "--------------------------------------------------\n",
      "Epoch 300\n",
      "Input Sequence:         [8, 7, 1, 7, 19, 4, 5, 13, 17, 17]\n",
      "Expected Sorted Output: [1, 4, 5, 7, 7, 8, 13, 17, 17, 19]\n",
      "Model Prediction:       [1, 4, 5, 7, 7, 8, 13, 17, 17, 19]\n",
      "Loss: 0.1588\n",
      "--------------------------------------------------\n",
      "Epoch 350\n",
      "Input Sequence:         [9, 4, 6, 7, 11, 15, 3, 2, 18, 5]\n",
      "Expected Sorted Output: [2, 3, 4, 5, 6, 7, 9, 11, 15, 18]\n",
      "Model Prediction:       [2, 3, 4, 5, 7, 7, 9, 11, 15, 18]\n",
      "Loss: 0.1444\n",
      "--------------------------------------------------\n",
      "Epoch 400\n",
      "Input Sequence:         [10, 6, 14, 6, 13, 14, 6, 17, 15, 16]\n",
      "Expected Sorted Output: [6, 6, 6, 10, 13, 14, 14, 15, 16, 17]\n",
      "Model Prediction:       [6, 6, 6, 10, 13, 14, 14, 15, 16, 17]\n",
      "Loss: 0.0951\n",
      "--------------------------------------------------\n",
      "Epoch 450\n",
      "Input Sequence:         [16, 10, 12, 3, 17, 6, 11, 13, 8, 7]\n",
      "Expected Sorted Output: [3, 6, 7, 8, 10, 11, 12, 13, 16, 17]\n",
      "Model Prediction:       [3, 6, 7, 8, 10, 11, 12, 13, 16, 17]\n",
      "Loss: 0.0474\n",
      "--------------------------------------------------\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "vocab_size = 20 \n",
    "seq_length = 10\n",
    "# Network hyperparameters\n",
    "embed_dim = 32\n",
    "num_layers = 2\n",
    "# Training hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 500\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = Transformer(vocab_size, embed_dim=embed_dim, seq_length=seq_length)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Generate a mini-batch for training\n",
    "    src, tgt = generate_data(batch_size, seq_length, vocab_size)\n",
    "    # Forward pass\n",
    "    output = model(src)\n",
    "    loss = criterion(output.flatten(0,1), tgt.flatten()) # Why flatten the output?\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Parameter updates\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print model output at each epoch\n",
    "    if epoch % 50 == 0 or epoch==10:\n",
    "        test_src, test_tgt = generate_data(batch_size, seq_length, vocab_size)\n",
    "        # test_pred should be a list of integers, the same length as test_src.\n",
    "        test_pred = model(test_src).argmax(dim=-1)\n",
    "\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\"Input Sequence:         {test_src.tolist()[0]}\")\n",
    "        print(f\"Expected Sorted Output: {test_tgt.tolist()[0]}\")\n",
    "        print(f\"Model Prediction:       {test_pred.tolist()[0]}\")\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence 1:         [16, 10, 12, 3, 17, 6, 11, 13, 8, 7]\n",
      "Expected Sorted Output 1: [3, 6, 7, 8, 10, 11, 12, 13, 16, 17]\n",
      "Model Prediction 1:       [3, 6, 7, 8, 10, 11, 12, 13, 16, 17]\n",
      "--------------------------------------------------\n",
      "Input Sequence 2:         [8, 5, 16, 13, 8, 9, 10, 5, 15, 9]\n",
      "Expected Sorted Output 2: [5, 5, 8, 8, 9, 9, 10, 13, 15, 16]\n",
      "Model Prediction 2:       [5, 5, 8, 8, 9, 9, 10, 13, 15, 16]\n",
      "--------------------------------------------------\n",
      "Input Sequence 3:         [18, 19, 17, 11, 14, 8, 9, 18, 19, 3]\n",
      "Expected Sorted Output 3: [3, 8, 9, 11, 14, 17, 18, 18, 19, 19]\n",
      "Model Prediction 3:       [3, 8, 9, 11, 14, 17, 18, 18, 19, 19]\n",
      "--------------------------------------------------\n",
      "Input Sequence 4:         [10, 9, 3, 10, 13, 10, 6, 1, 11, 9]\n",
      "Expected Sorted Output 4: [1, 3, 6, 9, 9, 10, 10, 10, 11, 13]\n",
      "Model Prediction 4:       [1, 3, 6, 9, 9, 10, 10, 10, 11, 13]\n",
      "--------------------------------------------------\n",
      "Input Sequence 5:         [19, 18, 4, 12, 17, 16, 3, 8, 11, 11]\n",
      "Expected Sorted Output 5: [3, 4, 8, 11, 11, 12, 16, 17, 18, 19]\n",
      "Model Prediction 5:       [3, 4, 8, 11, 11, 12, 16, 17, 18, 19]\n",
      "--------------------------------------------------\n",
      "Loss: 0.0693\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the model on 5 example sequences\n",
    "for i in range(5):\n",
    "    print(f\"Input Sequence {i + 1}:         {test_src.tolist()[i]}\")\n",
    "    print(f\"Expected Sorted Output {i + 1}: {test_tgt.tolist()[i]}\")\n",
    "    print(f\"Model Prediction {i + 1}:       {test_pred.tolist()[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Implement multi-headed attention\n",
    "\n",
    "<div style=\"max-width:400px\">\n",
    "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65c156ae-5cc5-4f7f-8652-dd5311b19beb_544x724.png\" alt=\"Transformer Architecture, with zoom on transformer layer\", \"width=\"50px\"\\>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__inita__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"Embedding size must be divisible by num_heads\"\n",
    "\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)  # Query projection\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)  # Key projection\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)  # Value projection\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "\n",
    "        # TODO: Compute Queries, Keys, Values\n",
    "        Q = self.W_q(x)  # (batch_size, seq_length, embed_dim)\n",
    "        K = self.W_k(x)  # (batch_size, seq_length, embed_dim)\n",
    "        V = self.W_v(x)  # (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        # Reshape for multiple heads\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # TODO: Apply scaled dot-product attention\n",
    "        # Dot-product similarities\n",
    "        scores = Q @ K.transpose(-2, -1) \n",
    "        # Scale by key dimension\n",
    "        scores /=  K.shape[-1] ** 0.5            \n",
    "        # Transform the scores into probabilities with the softmax function\n",
    "        scores = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Reshape and apply final linear layer\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_dim)\n",
    "        output = self.fc_out(attn_output)\n",
    "\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
