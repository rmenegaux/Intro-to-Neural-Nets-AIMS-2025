{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "- _Illustrations_ are from this great [blog post](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)\n",
    "- For _intuition_ on self-attention and LLMs, watch 3Blue1Brown animated videos [DL5](https://www.youtube.com/watch?v=wjZofJX0v4M) and [DL6](https://www.youtube.com/watch?v=eMlx5fFNoYc)\n",
    "- For a deep-dive, this [textbook chapter](https://web.stanford.edu/~jurafsky/slp3/9.pdf) for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformers: Lecture Notes**\n",
    "\n",
    "Transformers are a deep learning architecture introduced in the paper *\"Attention Is All You Need\"* (Vaswani et al., 2017). They are widely used in NLP, computer vision, and other AI applications.\n",
    "\n",
    "- Fully **attention-based**, replacing recurrence (RNNs) and convolution (CNNs).\n",
    "- Uses **self-attention** to model relationships between words in a sequence.\n",
    "- Processes sequences in **parallel**, enabling faster training.\n",
    "\n",
    "\n",
    "---\n",
    "## **I. Input Representation in Transformers**\n",
    "Transformers operate on a **sequence of vectors**. The input data must first be encoded as a sequence of numerical representations. \n",
    "\n",
    "### **I.1 Tokenization**\n",
    "For different types of data, tokenization methods vary:\n",
    "- **Text:** Splitting into words, subwords (e.g., BPE, WordPiece), or characters.\n",
    "- **Images:** Splitting into patches (e.g., ViTs use 16x16 pixel patches).\n",
    "- **Other Data:** Graphs, audio, and protein sequences have specialized tokenization approaches.\n",
    "\n",
    "### **I.1.1. Embedding Layer for Text**\n",
    "For text, each token is mapped to a **learnable embedding vector**:\n",
    "1. **Token Embeddings:** Each token is assigned a fixed-length vector.\n",
    "2. **Positional Encodings:** Since Transformers have no recurrence, position information is added explicitly.\n",
    "\n",
    "Given a vocabulary size $V$ and embedding dimension $d$, the embedding matrix is $E \\in \\mathbb{R}^{(V×d)}$, where each word index $i$ maps to $E[i]$.\n",
    "<div style=\"max-width:400px\">\n",
    "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fd4ac84-3925-428c-8f6a-64dfed5268ad_1714x848.png\" alt=\"Embedding layer\" />\n",
    "<div\\>\n",
    "\n",
    "### **I.2 Positional encodings**\n",
    "Transformers process input tokens **in parallel**, meaning they lack an inherent sense of token order. To distinguish sequences where the same words appear in different orders (e.g., *\"The cat chased the dog.\"* vs. *\"The dog chased the cat.\"*), **positional encodings** are added to token embeddings.  \n",
    "\n",
    "- The original **sinusoidal positional encoding** (Vaswani et al., 2017) assigns fixed patterns based on position $p$:\n",
    "$$PE_{(p, 2i)} = \\sin(p / 10000^{2i/d}) , \\quad PE_{(p, 2i+1)} = \\cos(p / 10000^{2i/d})$$\n",
    "\n",
    "- Alternatively, **learned positional embeddings** use a trainable lookup table (e.g., `nn.Embedding(max_length, d_model)`).  \n",
    "\n",
    "- Modern **LLMs use variants** like:  \n",
    "    - **ALiBi (Attention Linear Bias)**: Adds a decaying bias to attention scores.  \n",
    "    - **Rotary Positional Embeddings (RoPE)**: Rotates query/key vectors based on position for better generalization.  \n",
    "    - **T5-style relative positions**: Uses learned biases based on relative token distances.  \n",
    "\n",
    "The final input representation is the sum of **token embeddings** and **positional encodings**:\n",
    "\n",
    "$$X = E[tokens] + P[position]$$\n",
    "\n",
    "where $P[position]$ is a fixed or learned positional encoding matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## II. **Self-Attention Layer (Single-Head)**  \n",
    "\n",
    "#### **Intuition**  \n",
    "Self-attention allows each token in a sequence to attend to all other tokens, dynamically adjusting its representation based on their relevance. Instead of treating words independently, self-attention **mixes information** across tokens by computing a weighted sum of their embeddings.  \n",
    "\n",
    "<div style=\"max-width:400px\">\n",
    "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85c1f60b-8521-411b-9b84-54973929c251_1500x843.gif\" alt=\"Embedding layer\" />\n",
    "</div>\n",
    "\n",
    "Each token generates:  \n",
    "- A **query** vector ($Q$) – what it is looking for.  \n",
    "- A **key** vector ($K$) – how relevant it is to others.  \n",
    "- A **value** vector (\\(V\\)) – what information it carries.  \n",
    "\n",
    "Each token then gathers information from others based on how well its **query** matches their **keys**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Scalar Formulation**  \n",
    "For two tokens $i$ and $j$, their **attention score** is computed as:  \n",
    "\n",
    "$$\\text{score}(i, j) = \\frac{\\mathbf{q}_i \\cdot \\mathbf{k}_j}{\\sqrt{d_k}}$$\n",
    "\n",
    "where:  \n",
    "- $\\mathbf{q}_i$ is the query vector of token $i$.  \n",
    "- $\\mathbf{k}_j$ is the key vector of token $j$.  \n",
    "- $d_k$ is the scaling factor to keep scalar products within a reasonable range before application of the softmax.  \n",
    "\n",
    "The scores are normalized using **softmax**, producing attention weights:  \n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(\\text{score}(i, j))}{\\sum_k \\exp(\\text{score}(i, k))}\n",
    "$$\n",
    "\n",
    "Each token's final representation is the **weighted sum** of value vectors:  \n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i \\leftarrow \\sum_{j} \\alpha_{ij} \\mathbf{v}_j\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Matrix Formulation**  \n",
    "Given a sequence of $L$ tokens, represented as a matrix $X$ of shape $(L, d)$:  \n",
    "\n",
    "1. Compute **queries, keys, and values** using weight matrices:  \n",
    "\n",
    "   $$\n",
    "   Q = X W^Q, \\quad K = X W^K, \\quad V = X W^V\n",
    "   $$\n",
    "\n",
    "   where $W^Q, W^K, W^V$ are learnable weight matrices of shape $(d, d_k)$. \n",
    "\n",
    "2. Compute the **scaled attention scores**:  \n",
    "\n",
    "   $$\n",
    "   A = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "   where $A$ is an $(L \\times L)$ matrix of pairwise scores.  \n",
    "\n",
    "3. Apply **softmax** to get attention weights:  \n",
    "\n",
    "   $$\n",
    "   \\tilde{A} = \\text{softmax}(A)\n",
    "   $$\n",
    "\n",
    "4. Compute the **final output** as a weighted sum of values:  \n",
    "\n",
    "   $$\n",
    "   X = \\tilde{A} V\n",
    "   $$\n",
    "\n",
    "   where $Z$ is the updated representation of shape $(L, d_k)$.  \n",
    "\n",
    "This forms the **core operation** of a single **self-attention head**, which will later be extended to **multi-head attention**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)  # Query projection\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)  # Key projection\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)  # Value projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: (batch_size, seq_length, embed_dim)\n",
    "        Q = self.W_q(x)  # (batch_size, seq_length, embed_dim)\n",
    "        K = self.W_k(x)  # (batch_size, seq_length, embed_dim)\n",
    "        V = self.W_v(x)  # (batch_size, seq_length, embed_dim)\n",
    "        d_k = K.shape[-1] # Key dimension\n",
    "\n",
    "        # Dot-product similarities\n",
    "        scores = Q @ K.transpose(1, 2)\n",
    "        # Scale by dimension\n",
    "        scores /= d_k ** 0.5            \n",
    "        # Transform the scores into probabilities with the softmax function\n",
    "        scores = torch.softmax(scores, dim=-1)\n",
    "    \n",
    "        # Optional: store the attention weights for visualization\n",
    "        self.attention_weights = scores\n",
    "\n",
    "        # Update the vectors x\n",
    "        x = scores @ V\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-Head Attention\n",
    "\n",
    "### **Intuition**\n",
    "A single self-attention head learns a specific way to mix information across tokens. However, a single attention function might not be enough to capture different types of relationships in the data. **Multi-head attention** improves this by using multiple attention heads in parallel, each learning different attention patterns.\n",
    "\n",
    "Instead of computing a single set of **queries (Q), keys (K), and values (V)**, we compute multiple sets (one per head). Each head processes information differently, and their outputs are concatenated and linearly transformed to form the final representation.\n",
    "\n",
    "### **Formulation**\n",
    "Given an input matrix $X \\in \\mathbb{R}^{L \\times d}$ (where $L$ is the sequence length and $d$ is the embedding size), multi-head attention performs the following steps:\n",
    "\n",
    "1. **Compute multiple sets of Q, K, V**  \n",
    "   Each attention head has independent learned projection matrices $W^Q_h, W^K_h, W^V_h$:\n",
    "\n",
    "   $$\n",
    "   Q_h = X W^Q_h, \\quad K_h = X W^K_h, \\quad V_h = X W^V_h\n",
    "   $$\n",
    "\n",
    "   where $h$ indexes the head.\n",
    "\n",
    "2. **Compute self-attention for each head**  \n",
    "   Each head applies scaled dot-product attention independently:\n",
    "\n",
    "   $$\n",
    "   A_h = \\frac{Q_h K_h^T}{\\sqrt{d_k}}, \\quad \\tilde{A}_h = \\text{softmax}(A_h), \\quad Z_h = \\tilde{A}_h V_h\n",
    "   $$\n",
    "\n",
    "   The output of each head $Z_h$ has shape $(L, d_k)$.\n",
    "\n",
    "3. **Concatenate and project**  \n",
    "   The outputs from all $H$ heads are concatenated:\n",
    "\n",
    "   $$\n",
    "   Z = \\text{Concat}(Z_1, Z_2, ..., Z_H)\n",
    "   $$\n",
    "\n",
    "   Since each head outputs a vector of dimension $d_k$, the concatenated representation has shape $(L, H \\cdot d_k)$. We then apply a final linear transformation with weight matrix $W^O \\in \\mathbb{R}^{(H \\cdot d_k) \\times d}$:\n",
    "\n",
    "   $$\n",
    "   Z_{\\text{final}} = Z W^O\n",
    "   $$\n",
    "\n",
    "\n",
    "<div style=\"max-width:300px\">\n",
    "<figure>\n",
    "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65c156ae-5cc5-4f7f-8652-dd5311b19beb_544x724.png\" alt=\"Embedding layer\" />\n",
    "    <figcaption>Multi-head attention: each head performs self-attention in parallel. Outputs are concatenated.</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "### **Key Takeaways**\n",
    "- Multiple attention heads allow the model to capture different relationships between tokens.\n",
    "- Each head computes independent self-attention, but the results are combined into a single representation.\n",
    "- This technique improves the expressiveness of transformers without increasing computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## III. Transformer Layer and Transformer Network\n",
    "\n",
    "A **Transformer layer** consists of two main components:  \n",
    "1. **Self-attention mixing** – Updates each token representation by attending to all other tokens.  \n",
    "2. **Token-wise transformations** – Applies feedforward transformations to each token independently.\n",
    "<div style=\"max-width:600px\">\n",
    "<figure>\n",
    "    <img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21b82064-554d-43d2-b6f1-589090e830ce_1294x410.png\" alt=\"Tokenwise transformation\" />\n",
    "    <figcaption>Tokenwise transformation: the _same_ network is applied to each token independently</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "\n",
    "To stabilize training, **residual connections** and **layer normalization** are applied after both self-attention and feedforward transformations.\n",
    "\n",
    "<div style=\"max-width:300px\">\n",
    "<figure>\n",
    "    <img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5024bcc5-33c9-4d53-9bd7-56cbcf9c4627_874x1108.png\" alt=\"Transformer Layer\" />\n",
    "    <figcaption>Transformer layer architecture</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "\n",
    "A **Transformer network** is built by **stacking multiple Transformer layers**, allowing deeper contextual representations to emerge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Task-Specific Processing After Transformers\n",
    "\n",
    "Once the Transformer has computed **rich contextualized vectors**, the next step depends on the specific task. Different types of processing heads can be applied:\n",
    "\n",
    "1. **Single prediction (sequence-level classification)**:  \n",
    "    - A **pooling operation** (e.g., mean/max pooling over all token embeddings), or\n",
    "    - a **single special token** (e.g., `[CLS]` in BERT) is used to represent the entire sequence.  \n",
    "    - **Examples**: Sentiment analysis, document classification.\n",
    "\n",
    "2. **One-to-One Prediction (Token-Level Classification)**\n",
    "    - A classifier is applied **independently to each token's representation**.  \n",
    "    - **Example**: Sorting a sequence of numbers, Named Entity Recognition (NER), next-token prediction (at training).\n",
    "    <div style=\"max-width:500px\">\n",
    "    <figure>\n",
    "        <img src=\"\n",
    "        https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cad60e3-cec7-4bfa-9ad1-356b6d181f7c_1640x862.png\" alt=\"Transformer Layer\" />\n",
    "        <figcaption>Classification head, with one output per token</figcaption>\n",
    "    </figure>\n",
    "    </div>\n",
    "\n",
    "3. **Sequence-to-Sequence (Decoder-Based Tasks)**\n",
    "    - The transformer generates an **output sequence** based on an **input sequence**.  \n",
    "    - **Decoder mechanism**: Uses both self-attention and cross-attention (attending to encoder outputs).  \n",
    "    - **Examples**: Machine translation (English → French), text summarization.\n",
    "\n",
    "Each task type leverages the **same underlying Transformer architecture**, with different output processing layers tailored to the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Attention\n",
    "\n",
    "Cross-attention extends the self-attention mechanism by allowing queries, keys, and values to come from **different sequences**. This is particularly useful in **encoder-decoder architectures**, where the decoder attends to the encoder's output.\n",
    "\n",
    "### **Key Difference from Self-Attention**\n",
    "- In **self-attention**, queries, keys, and values all come from the **same** sequence.\n",
    "- In **cross-attention**, queries come from the **decoder**, while keys and values come from the **encoder**.\n",
    "\n",
    "### **Key Property**\n",
    "- The number of **queries** does not have to match the number of **keys**.  \n",
    "- This allows attention to be computed **between sequences of different lengths**.\n",
    "- **Example**: In machine translation, the input sentence (source) may have a different length than the output sentence (target).\n",
    "\n",
    "### **Formulation**\n",
    "Given:\n",
    "- **Decoder queries**: $Q \\in \\mathbb{R}^{n_{\\text{dec}} \\times d}$\n",
    "- **Encoder keys/values**: $K, V \\in \\mathbb{R}^{n_{\\text{enc}} \\times d}$\n",
    "\n",
    "The attention scores are computed as:\n",
    "\n",
    "$$\n",
    "A = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "where $A \\in \\mathbb{R}^{n_{\\text{dec}} \\times n_{\\text{enc}}}$ captures the relevance of each encoder token to each decoder token.\n",
    "\n",
    "Applying softmax and weighting by $V$:\n",
    "\n",
    "$$\n",
    "Z = \\text{softmax}(A) V\n",
    "$$\n",
    "\n",
    "where $Z \\in \\mathbb{R}^{n_{\\text{dec}} \\times d}$ is the updated decoder representation.\n",
    "\n",
    "This mechanism allows the decoder to focus on the most relevant encoder tokens at each step, enabling **context-aware generation** in sequence-to-sequence tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Language Modeling (MLM)\n",
    "\n",
    "**Masked Language Modeling (MLM)** is a training objective where a model learns to predict missing words in a sentence. This is a **one-to-one prediction task**, where each token is processed independently, but only some tokens are used for supervision.\n",
    "\n",
    "### **How It Works**\n",
    "1. **Randomly mask** some tokens in the input sequence (e.g., 15% of tokens in BERT).\n",
    "2. **Pass the masked sequence** through the Transformer.\n",
    "3. **Predict the original tokens** for the masked positions using a classifier on top of each token's representation.\n",
    "4. **Loss is only computed** on masked tokens, ignoring others.\n",
    "\n",
    "### **Example**\n",
    "**Input:**  \n",
    "*\"The cat sat on the [MASK].\"*  \n",
    "**Target:**  \n",
    "*\"The cat sat on the mat.\"*\n",
    "\n",
    "### **Formulation**\n",
    "Let $X = (x_1, x_2, ..., x_n)$ be the input tokens. A subset is masked, creating $X'$.  \n",
    "The model computes contextualized embeddings for all tokens:\n",
    "\n",
    "$$\n",
    "Z = \\text{Transformer}(X')\n",
    "$$\n",
    "\n",
    "A classification layer predicts the masked words:\n",
    "\n",
    "$$\n",
    "p(x_i) = \\text{softmax}(W Z_i)\n",
    "$$\n",
    "\n",
    "where $W$ maps hidden states to vocabulary logits.\n",
    "\n",
    "### **Why MLM?**\n",
    "- Enables **bidirectional context learning**, unlike traditional left-to-right language modeling.\n",
    "- Pretraining with MLM improves performance on **downstream NLP tasks** like classification, QA, and summarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
