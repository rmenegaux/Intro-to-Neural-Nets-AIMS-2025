{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise: Two-Neuron ReLU Network and Gradient Descent\n",
    "\n",
    "#### Given:\n",
    "- Activation function: $\\sigma(x) = \\max(0, x)$ (ReLU)  \n",
    "- Input $x \\in \\mathbb{R}$\n",
    "- Neural network with two neurons:\n",
    "   - $w_1 = 1, b_1 = -1$\n",
    "   - $w_2 = -1, b_2 = 0$\n",
    "\n",
    "Output function:\n",
    "$$\\hat{y}(x) = \\sigma(w_1 x + b_1) + \\sigma(w_2 x + b_2) \\in \\mathbb{R}$$\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "1. **Plot the output function**  \n",
    "   Plot $\\hat{y}(x)$ as a function of $x$.\n",
    "\n",
    "2. **Stochastic Gradient Descent with Sample**  \n",
    "   Given a training sample $(x_0, y_0) = (-0.5, 1)$:\n",
    "\n",
    "- **(2a)** Plot the sample $(x_0, y_0)$ on the previous plot.\n",
    "- **(2b)** Determine the direction in which the weights $w_1, w_2$ and biases $b_1, b_2$ will move after a gradient step. Justify your answer without performing full gradient calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise: FLOPs and Parameters\n",
    "\n",
    "#### **FLOP Definition**  \n",
    "A **FLOP (Floating Point Operation)** is a basic arithmetic operation (e.g., addition or multiplication) performed on scalar values.\n",
    "\n",
    "#### **Tasks**  \n",
    "1. **Scalar Product**  \n",
    "   Given two vectors $a, b \\in \\mathbb{R}^d$, compute the number of FLOPs required to compute their dot product.\n",
    "\n",
    "2. **Linear Transformation**  \n",
    "   Consider a fully connected (linear) layer with input dimension $d$ and output dimension $d'$ (denoted as `Linear(d, d')`):\n",
    "   - How many **parameters** (weights) does this layer have (assuming no bias)?\n",
    "   - How many **FLOPs** are required for a single forward pass through this layer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **General NN Questions**\n",
    "1. Explain the vanishing/exploding gradient problem in training neural networks.\n",
    "2. Why are normalization layers used?\n",
    "3. Why are residual connections used?\n",
    "4. Why is weight initialization important? Give an example with a linear layer.\n",
    "5. What is the link between a Linear layer and a convolutional layer?\n",
    "\n",
    "### **Transformer questions**\n",
    "1. What is the purpose of self-attention in a transformer model?\n",
    "2. Why do we scale the dot-product attention scores by \\( \\sqrt{d}$?\n",
    "3. Why do we apply the **softmax function** in the attention mechanism?\n",
    "4. How are the parameters of the $W_Q, W_K, W_V$ projection matrices set?\n",
    "5. What is the time complexity of the self-attention mechanism?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
